{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a0c2f23",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f222b701",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1821a745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1770d337",
   "metadata": {},
   "source": [
    "## Step 2: Dataset Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "79ae7833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purposes of this tutorial, we'll fetch the shakespeare data set from the google api\n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "# the arguments to the above would be changed for our project since we'd have our own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed939eae",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "673fd9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, open the file\n",
    "f = open(path_to_file, 'rb')\n",
    "# Now, we read the raw file data\n",
    "raw_data = f.read()\n",
    "# Next, decode the data\n",
    "decoded_text = raw_data.decode(encoding='utf-8')\n",
    "\n",
    "# We should also make a set of the unique characters in the set\n",
    "vocab = sorted(set(decoded_text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "55708906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text in characters: 1115394\n",
      "\n",
      "First 250 chars:\n",
      "\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# As a makeshift sanity test, let's print the length of the text and the first 250 characters\n",
    "print(\"Length of text in characters:\", len(decoded_text))\n",
    "print(\"\\nFirst 250 chars:\\n\")\n",
    "print(decoded_text[:250])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a063540",
   "metadata": {},
   "source": [
    "# Process the Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af47c09",
   "metadata": {},
   "source": [
    "## Vectorize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36cd1c",
   "metadata": {},
   "source": [
    "We need to find a way to numerically represent the text\n",
    "\n",
    "Tensorflow's `keras.layers.StringLookup` can get a numerical representation of a character, but we first need to split up the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3c79b756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split up string into chars\n",
    "chars = tf.strings.unicode_split(decoded_text, input_encoding='UTF-8')\n",
    "\n",
    "# Next, use keras.layers.StringLookup to numerically represent the characters\n",
    "# To do this, we need to create the String Lookup layer; this is a preprocessing layer\n",
    "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)\n",
    "# Now we can get the ids from characters\n",
    "ids = ids_from_chars(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7755823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to be able to invert the above process, so we do the following:\n",
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)\n",
    "\n",
    "chars = chars_from_ids(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c38f06f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can define the following function to join characters back into strings\n",
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199580e6",
   "metadata": {},
   "source": [
    "## Training Examples and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889ad3b",
   "metadata": {},
   "source": [
    "Let's divide the text into example sequences, each of which has `seq_length` characters\n",
    "Each input sequence -> corresponding targets contain same length of text shifted down by 1\n",
    "Example: if we have a string \"hello\" with `seq_length = 4`, input sequence is \"hell\" and corresponding target is \"ello\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a5955fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, convert text vector into stream of character indices\n",
    "all_ids = ids_from_chars(tf.strings.unicode_split(decoded_text, 'UTF-8'))\n",
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01534298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dad4eddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seq_length\n",
    "seq_length = 100\n",
    "# use batch method to convert individual characters to sequences of desired size\n",
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "871fc52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need input/label pairs for training, so we use this function\n",
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00bc7482",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the map function and the above to create the following\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04cd5f8",
   "metadata": {},
   "source": [
    "## Create Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "da7618de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Need a buffer in which dataset shuffling may occur\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd30563",
   "metadata": {},
   "source": [
    "# Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820e2b4",
   "metadata": {},
   "source": [
    "We need to define a model with three layers:\n",
    "1. Input layer: a trainable lookup table that will map each character-ID to a vector with `embedding-dim` dimensions\n",
    "2. `tf.keras.layer.GRU`: a type of RNN with size `units=rnn_units`\n",
    "3. `tf.keras.layers.Dense`: output layer (the number of outputs is `vocab_size`). It outputs one 'logit' for each character in vocab. According to the model, these are the long-likelihood of each character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8bc04385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to define some variables\n",
    "\n",
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d4e3f607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's make a model class\n",
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "        super().__init__(self)\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                      return_sequences=True,\n",
    "                                      return_state=True)\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    def call(self, inputs, states=None, return_state = False, training = False):\n",
    "        x = inputs\n",
    "        x = self.embedding(x, training=training)\n",
    "        if states is None:\n",
    "            states = self.gru.get_initial_state(x)\n",
    "        x, states = self.gru(x, initial_state=states, training=training)\n",
    "\n",
    "        x = self.dense(x, training=training)\n",
    "\n",
    "        if return_state:\n",
    "          return x, states\n",
    "        else:\n",
    "          return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7d5fab7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dcc097",
   "metadata": {},
   "source": [
    "According to the tutorial:\n",
    "> For each character the model looks up the embedding, runs the GRU one timestep with the embedding as input, and applies the dense layer to generate logits predicting the log-likelihood of the next character"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2368b7b5",
   "metadata": {},
   "source": [
    "# Trying the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63f5f314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "# Let's check the shape of the model\n",
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e2cb992d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  16896     \n",
      "                                                                 \n",
      " gru_1 (GRU)                 multiple                  3938304   \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d88c7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get predictions of the model by sampling from the output distribution\n",
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "001d44d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 32  6 51 48 18 33 22 61 39 28  6  9 11 12 62 22 24 61  1 52  3 53 61\n",
      " 62  2 10 24 32 36 33 16 40 17 39 25 61 16 26 55 23 34 48 50 32 46  4  7\n",
      " 59 49 36 53  1 30 20 49 31 58 55 15 52 50 37 44 54 38 17 53 53  6 37 18\n",
      " 57 26 37 63 13 10 38 32 13 23 53 31 25 16 37 45 64 15 26 45 55  1 60 39\n",
      " 43 39 17 60]\n"
     ]
    }
   ],
   "source": [
    "# We now have a prediction of the next character index for each timestep\n",
    "print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cd684a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      "b\"s summer by this sun of York;\\nAnd all the clouds that lour'd upon our house\\nIn the deep bosom of the\"\n",
      "Model:\n",
      "b\"FS'liETIvZO'.:;wIKv\\nm!nvw 3KSWTCaDZLvCMpJUikSg$,tjWn\\nQGjRspBmkXeoYDnn'XErMXx?3YS?JnRLCXfyBMfp\\nuZdZDu\"\n"
     ]
    }
   ],
   "source": [
    "# Let's print the actual next character compared to what the model says\n",
    "print(\"Input:\")\n",
    "print(text_from_ids(input_example_batch[0]).numpy())\n",
    "print(\"Model:\")\n",
    "print(text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab963e30",
   "metadata": {},
   "source": [
    "Clearly, the model isn't doing too great. So let's move on to training it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d1404e",
   "metadata": {},
   "source": [
    "# Training the Data\n",
    "The first step in training the data is adding a loss function\n",
    "The standard `tf.keras.sparse_categorical_crossentropy` serves our purpose fairly well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6297e78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be8802af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.191824, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "feb394ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the loss function and tf.keras.optimizer.Adam, we can use configure training\n",
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7c842460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can configure it so that checkpoints are saved during training\n",
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "793c00b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "172/172 [==============================] - 141s 816ms/step - loss: 2.7407\n",
      "Epoch 2/10\n",
      "172/172 [==============================] - 146s 849ms/step - loss: 2.0128\n",
      "Epoch 3/10\n",
      "172/172 [==============================] - 145s 840ms/step - loss: 1.7337\n",
      "Epoch 4/10\n",
      "172/172 [==============================] - 143s 830ms/step - loss: 1.5684\n",
      "Epoch 5/10\n",
      "172/172 [==============================] - 146s 848ms/step - loss: 1.4669\n",
      "Epoch 6/10\n",
      "172/172 [==============================] - 144s 834ms/step - loss: 1.3979\n",
      "Epoch 7/10\n",
      "172/172 [==============================] - 146s 847ms/step - loss: 1.3449\n",
      "Epoch 8/10\n",
      "172/172 [==============================] - 148s 856ms/step - loss: 1.3004\n",
      "Epoch 9/10\n",
      "172/172 [==============================] - 147s 854ms/step - loss: 1.2595\n",
      "Epoch 10/10\n",
      "172/172 [==============================] - 146s 848ms/step - loss: 1.2207\n"
     ]
    }
   ],
   "source": [
    "# Now, we execute training\n",
    "\n",
    "# Set epochs of training\n",
    "EPOCHS = 10\n",
    "\n",
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a76d8",
   "metadata": {},
   "source": [
    "# Generate Text\n",
    "The above training takes a while to execute, but once it's done, we can generate text!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "183dc15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e12ec350",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "96d2c27f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Heaven and did son, espare defend.\n",
      "\n",
      "GLOUCESTER:\n",
      "\n",
      "KING EDWARD IV:\n",
      "To thee,\n",
      "That trues are illest eat: and there by thee\n",
      "as he would give not known inprinity.\n",
      "That would be look'd with her, or they\n",
      "were soon so slain?\n",
      "Once more of you to cry,\n",
      "Dentile thee; thousand fiery?\n",
      "\n",
      "GONZALO:\n",
      "You duke, do not witness that the sentence\n",
      "That should not burill in our\n",
      "fainth: some it mine enemy?\n",
      "I think, thy free ut tafen,\n",
      "The sostions a happy moletainted\n",
      "Though I the earth; and to 't in hope to you to pass,\n",
      "And fit her elentage, and as to grant\n",
      "Hereafter'd of nothing gizent\n",
      "To saint in refender to chy horses! Ofthing early him\n",
      "To hize as wencefter in my jogs,\n",
      "My nature you cannot army,\n",
      "Ground so carry gives, married to the Tircules\n",
      "Of enbert'st the issue; and no more but that about him;\n",
      "So and dost nature scope their work:\n",
      "Why, leth he of King Richard.\n",
      "\n",
      "DUKE OF AUMERLE:\n",
      "You have to-four among a father.\n",
      "\n",
      "GONZALO:\n",
      "That's a greater grow it think for care.\n",
      "Mountant the banished? O, methoughts, and I conc \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 0.850712776184082\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac5de45",
   "metadata": {},
   "source": [
    "# Export Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c218cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x144fbf550>, because it is not built.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_1_layer_call_fn, gru_cell_1_layer_call_and_return_conditional_losses while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step/assets\n"
     ]
    }
   ],
   "source": [
    "# We can save the model like so\n",
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "976e766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Come, good arm them; every thousand men,\n",
      "Before a set how his prove as hate me,\n",
      "If you ragker cause\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9dbb6b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
